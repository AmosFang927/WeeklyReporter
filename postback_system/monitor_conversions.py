#!/usr/bin/env python3
"""
è½¬åŒ–æ•°æ®å®æ—¶ç›‘æ§ç³»ç»Ÿ
é˜²æ­¢æ•°æ®ä¸¢å¤±ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
"""

import asyncio
import asyncpg
import subprocess
import re
import json
import logging
import time
from datetime import datetime, timedelta
from urllib.parse import unquote
import schedule
import threading
import sys

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/conversion_monitor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“è¿æ¥é…ç½®
DATABASE_CONFIGS = [
    "postgresql://postback_admin:ByteC2024PostBack_CloudSQL_20250708@/postback_db?host=/cloudsql/solar-idea-463423-h8:asia-southeast1:bytec-postback-db&sslmode=require",
    "postgresql://postback_admin:ByteC2024PostBack_CloudSQL_20250708@34.124.206.16:5432/postback_db",
    "postgresql://postback_admin:ByteC2024PostBack_CloudSQL_20250708@10.82.0.3:5432/postback_db"
]

class ConversionMonitor:
    def __init__(self):
        self.db_url = None
        self.last_check_time = datetime.now()
        self.alert_thresholds = {
            'no_data_minutes': 30,  # 30åˆ†é’Ÿæ— æ•°æ®å‘Šè­¦
            'error_rate_percent': 10,  # é”™è¯¯ç‡è¶…è¿‡10%å‘Šè­¦
            'db_connection_failures': 3  # è¿ç»­3æ¬¡æ•°æ®åº“è¿æ¥å¤±è´¥å‘Šè­¦
        }
        self.consecutive_db_failures = 0
        
    async def init_database_connection(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ” åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        
        for i, db_url in enumerate(DATABASE_CONFIGS, 1):
            try:
                logger.info(f"å°è¯•è¿æ¥æ–¹å¼ {i}...")
                conn = await asyncpg.connect(db_url)
                await conn.fetchval("SELECT 1")
                await conn.close()
                logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: æ–¹å¼ {i}")
                self.db_url = db_url
                self.consecutive_db_failures = 0
                return True
            except Exception as e:
                logger.warning(f"âŒ è¿æ¥å¤±è´¥ {i}: {str(e)}")
        
        self.consecutive_db_failures += 1
        logger.error(f"âŒ æ‰€æœ‰æ•°æ®åº“è¿æ¥æ–¹å¼éƒ½å¤±è´¥ (è¿ç»­å¤±è´¥: {self.consecutive_db_failures})")
        return False

    async def check_database_health(self):
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        if not self.db_url:
            return False
            
        try:
            conn = await asyncpg.connect(self.db_url)
            
            # æ£€æŸ¥è¿æ¥
            await conn.fetchval("SELECT 1")
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            tables_exist = await conn.fetchval("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_name IN ('conversions', 'tenants')
            """)
            
            # æ£€æŸ¥æœ€è¿‘æ•°æ®
            recent_count = await conn.fetchval("""
                SELECT COUNT(*) FROM conversions 
                WHERE received_at > NOW() - INTERVAL '1 hour'
            """)
            
            await conn.close()
            
            health_status = {
                'connection': True,
                'tables_exist': tables_exist == 2,
                'recent_conversions': recent_count,
                'timestamp': datetime.now()
            }
            
            logger.info(f"ğŸ’Š æ•°æ®åº“å¥åº·æ£€æŸ¥: è¿æ¥æ­£å¸¸, è¡¨å­˜åœ¨: {health_status['tables_exist']}, "
                       f"æœ€è¿‘1å°æ—¶è½¬åŒ–: {recent_count}")
            
            return health_status
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def get_recent_logs(self, minutes=60):
        """è·å–æœ€è¿‘çš„è½¬åŒ–æ—¥å¿—"""
        logger.info(f"ğŸ“„ è·å–æœ€è¿‘ {minutes} åˆ†é’Ÿçš„è½¬åŒ–æ—¥å¿—...")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        start_time = datetime.now() - timedelta(minutes=minutes)
        start_time_str = start_time.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        cmd = [
            'gcloud', 'logging', 'read',
            f'resource.type="cloud_run_revision" AND resource.labels.service_name="bytec-public-postback" AND textPayload:"/involve/event" AND timestamp>="{start_time_str}"',
            '--limit=1000',
            '--freshness=1h',
            '--format=value(timestamp,textPayload)',
            '--project', 'solar-idea-463423-h8'
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                logs = []
                for line in result.stdout.strip().split('\n'):
                    if line.strip() and '/involve/event' in line:
                        logs.append(line.strip())
                
                logger.info(f"âœ… è·å–åˆ° {len(logs)} æ¡æ—¥å¿—è®°å½•")
                return logs
            else:
                logger.error(f"âŒ è·å–æ—¥å¿—å¤±è´¥: {result.stderr}")
                return []
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ è·å–æ—¥å¿—è¶…æ—¶")
            return []
        except Exception as e:
            logger.error(f"âŒ è·å–æ—¥å¿—å¼‚å¸¸: {str(e)}")
            return []

    def parse_conversion_from_log(self, log_line):
        """ä»æ—¥å¿—è¡Œè§£æè½¬åŒ–æ•°æ®"""
        try:
            # æå–æ—¶é—´æˆ³
            timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})', log_line)
            timestamp = None
            if timestamp_match:
                timestamp = datetime.fromisoformat(timestamp_match.group(1))
            
            # æå–URLå‚æ•°
            url_pattern = r'GET /involve/event\?([^"]+)'
            match = re.search(url_pattern, log_line)
            
            if not match:
                return None
            
            query_string = match.group(1)
            params = {}
            
            for param_pair in query_string.split('&'):
                if '=' in param_pair:
                    key, value = param_pair.split('=', 1)
                    params[key] = unquote(value)
            
            # æ„å»ºè½¬åŒ–æ•°æ®
            conversion = {
                'conversion_id': params.get('conversion_id', ''),
                'offer_name': params.get('offer_name', ''),
                'usd_sale_amount': float(params.get('usd_sale_amount', 0)),
                'usd_payout': float(params.get('usd_payout', 0)),
                'sub_id': params.get('sub_id', ''),
                'media_id': params.get('media_id', ''),
                'received_at': timestamp or datetime.now(),
                'raw_data': json.dumps(params)
            }
            
            return conversion
            
        except Exception as e:
            logger.error(f"è§£ææ—¥å¿—å¤±è´¥: {str(e)} - {log_line[:100]}")
            return None

    async def check_missing_conversions(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜åˆ°æ•°æ®åº“çš„è½¬åŒ–æ•°æ®"""
        logger.info("ğŸ” æ£€æŸ¥ç¼ºå¤±çš„è½¬åŒ–æ•°æ®...")
        
        # è·å–æœ€è¿‘60åˆ†é’Ÿçš„æ—¥å¿—
        logs = self.get_recent_logs(60)
        if not logs:
            logger.warning("âš ï¸ æ— æ³•è·å–æ—¥å¿—æ•°æ®")
            return {'status': 'warning', 'message': 'æ— æ³•è·å–æ—¥å¿—æ•°æ®'}
        
        # è§£æè½¬åŒ–æ•°æ®
        log_conversions = {}
        for log in logs:
            conversion = self.parse_conversion_from_log(log)
            if conversion and conversion['conversion_id']:
                log_conversions[conversion['conversion_id']] = conversion
        
        logger.info(f"ğŸ“Š æ—¥å¿—ä¸­å‘ç° {len(log_conversions)} ä¸ªå”¯ä¸€è½¬åŒ–")
        
        if not log_conversions:
            logger.info("âœ… æœ€è¿‘60åˆ†é’Ÿæ— è½¬åŒ–æ•°æ®")
            return {'status': 'ok', 'message': 'æœ€è¿‘60åˆ†é’Ÿæ— è½¬åŒ–æ•°æ®'}
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„è½¬åŒ–æ•°æ®
        if not self.db_url:
            logger.error("âŒ æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•æ¯”è¾ƒ")
            return {'status': 'error', 'message': 'æ•°æ®åº“æœªè¿æ¥'}
        
        try:
            conn = await asyncpg.connect(self.db_url)
            
            # è·å–æœ€è¿‘60åˆ†é’Ÿæ•°æ®åº“ä¸­çš„è½¬åŒ–ID
            db_conversion_ids = await conn.fetch("""
                SELECT conversion_id FROM conversions 
                WHERE received_at > NOW() - INTERVAL '60 minutes'
            """)
            
            db_ids = set(row['conversion_id'] for row in db_conversion_ids)
            log_ids = set(log_conversions.keys())
            
            # æ‰¾å‡ºç¼ºå¤±çš„è½¬åŒ–
            missing_ids = log_ids - db_ids
            
            await conn.close()
            
            if missing_ids:
                logger.warning(f"âš ï¸ å‘ç° {len(missing_ids)} ä¸ªç¼ºå¤±çš„è½¬åŒ–æ•°æ®")
                return {
                    'status': 'missing_data',
                    'missing_count': len(missing_ids),
                    'missing_conversions': [log_conversions[cid] for cid in missing_ids],
                    'message': f'å‘ç° {len(missing_ids)} ä¸ªç¼ºå¤±çš„è½¬åŒ–æ•°æ®'
                }
            else:
                logger.info("âœ… æ‰€æœ‰è½¬åŒ–æ•°æ®éƒ½å·²ä¿å­˜åˆ°æ•°æ®åº“")
                return {'status': 'ok', 'message': 'æ‰€æœ‰è½¬åŒ–æ•°æ®éƒ½å·²ä¿å­˜'}
                
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç¼ºå¤±æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return {'status': 'error', 'message': f'æ£€æŸ¥å¤±è´¥: {str(e)}'}

    async def recover_missing_conversions(self, missing_conversions):
        """æ¢å¤ç¼ºå¤±çš„è½¬åŒ–æ•°æ®"""
        logger.info(f"ğŸ”„ å¼€å§‹æ¢å¤ {len(missing_conversions)} ä¸ªç¼ºå¤±çš„è½¬åŒ–...")
        
        if not self.db_url:
            logger.error("âŒ æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•æ¢å¤")
            return False
        
        try:
            conn = await asyncpg.connect(self.db_url)
            
            success_count = 0
            for conversion in missing_conversions:
                try:
                    await conn.execute("""
                        INSERT INTO conversions (
                            conversion_id, offer_name, usd_sale_amount, usd_payout,
                            sub_id, media_id, raw_data, received_at
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (conversion_id) DO NOTHING
                    """,
                        conversion['conversion_id'],
                        conversion['offer_name'],
                        conversion['usd_sale_amount'],
                        conversion['usd_payout'],
                        conversion['sub_id'],
                        conversion['media_id'],
                        conversion['raw_data'],
                        conversion['received_at']
                    )
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ æ¢å¤è½¬åŒ–å¤±è´¥ {conversion['conversion_id']}: {str(e)}")
            
            await conn.close()
            
            logger.info(f"âœ… æˆåŠŸæ¢å¤ {success_count}/{len(missing_conversions)} ä¸ªè½¬åŒ–")
            return success_count > 0
            
        except Exception as e:
            logger.error(f"âŒ æ¢å¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False

    async def send_alert(self, alert_type, message, details=None):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        alert_data = {
            'timestamp': datetime.now().isoformat(),
            'type': alert_type,
            'message': message,
            'details': details or {}
        }
        
        # è®°å½•åˆ°æ—¥å¿—
        logger.error(f"ğŸš¨ ALERT [{alert_type}]: {message}")
        
        # å¯ä»¥åœ¨æ­¤å¤„æ·»åŠ é‚®ä»¶ã€Slackç­‰é€šçŸ¥æ–¹å¼
        # ç›®å‰åªè®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        with open('/tmp/conversion_alerts.log', 'a') as f:
            f.write(f"{json.dumps(alert_data)}\n")

    async def monitor_cycle(self):
        """å•æ¬¡ç›‘æ§å‘¨æœŸ"""
        logger.info("ğŸ”„ å¼€å§‹ç›‘æ§å‘¨æœŸ...")
        
        try:
            # 1. æ£€æŸ¥æ•°æ®åº“è¿æ¥
            if not await self.init_database_connection():
                if self.consecutive_db_failures >= self.alert_thresholds['db_connection_failures']:
                    await self.send_alert(
                        'database_connection_failure',
                        f'æ•°æ®åº“è¿ç»­ {self.consecutive_db_failures} æ¬¡è¿æ¥å¤±è´¥',
                        {'consecutive_failures': self.consecutive_db_failures}
                    )
                return
            
            # 2. æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€
            health = await self.check_database_health()
            if not health:
                await self.send_alert('database_health_check_failed', 'æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥')
                return
            
            # 3. æ£€æŸ¥ç¼ºå¤±çš„è½¬åŒ–æ•°æ®
            missing_check = await self.check_missing_conversions()
            
            if missing_check['status'] == 'missing_data':
                # å°è¯•è‡ªåŠ¨æ¢å¤
                success = await self.recover_missing_conversions(missing_check['missing_conversions'])
                
                if success:
                    logger.info(f"âœ… è‡ªåŠ¨æ¢å¤äº† {missing_check['missing_count']} ä¸ªç¼ºå¤±çš„è½¬åŒ–")
                else:
                    await self.send_alert(
                        'data_recovery_failed',
                        f"å‘ç° {missing_check['missing_count']} ä¸ªç¼ºå¤±è½¬åŒ–ï¼Œè‡ªåŠ¨æ¢å¤å¤±è´¥",
                        missing_check
                    )
            
            # 4. æ£€æŸ¥æ— æ•°æ®æƒ…å†µ
            if isinstance(health, dict) and health.get('recent_conversions', 0) == 0:
                time_since_last_data = datetime.now() - self.last_check_time
                if time_since_last_data.total_seconds() > self.alert_thresholds['no_data_minutes'] * 60:
                    await self.send_alert(
                        'no_recent_conversions',
                        f'è¶…è¿‡ {self.alert_thresholds["no_data_minutes"]} åˆ†é’Ÿæ— è½¬åŒ–æ•°æ®',
                        {'last_check': self.last_check_time.isoformat()}
                    )
            else:
                self.last_check_time = datetime.now()
            
            logger.info("âœ… ç›‘æ§å‘¨æœŸå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç›‘æ§å‘¨æœŸå¼‚å¸¸: {str(e)}")
            await self.send_alert('monitor_system_error', f'ç›‘æ§ç³»ç»Ÿå¼‚å¸¸: {str(e)}')

    async def run_continuous_monitoring(self):
        """æŒç»­ç›‘æ§"""
        logger.info("ğŸš€ å¯åŠ¨è½¬åŒ–æ•°æ®æŒç»­ç›‘æ§...")
        
        while True:
            try:
                await self.monitor_cycle()
                # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                await asyncio.sleep(300)
                
            except KeyboardInterrupt:
                logger.info("ğŸ‘‹ ç›‘æ§ç³»ç»Ÿæ­£å¸¸é€€å‡º")
                break
            except Exception as e:
                logger.error(f"âŒ ç›‘æ§ç³»ç»Ÿå¼‚å¸¸: {str(e)}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­

    def run_scheduled_monitoring(self):
        """å®šæ—¶ç›‘æ§æ¨¡å¼"""
        logger.info("ğŸ“… å¯åŠ¨å®šæ—¶ç›‘æ§æ¨¡å¼...")
        
        # æ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡
        schedule.every(5).minutes.do(lambda: asyncio.run(self.monitor_cycle()))
        
        # æ¯å°æ—¶ç”ŸæˆæŠ¥å‘Š
        schedule.every().hour.do(lambda: asyncio.run(self.generate_hourly_report()))
        
        while True:
            try:
                schedule.run_pending()
                time.sleep(30)
            except KeyboardInterrupt:
                logger.info("ğŸ‘‹ å®šæ—¶ç›‘æ§æ­£å¸¸é€€å‡º")
                break

    async def generate_hourly_report(self):
        """ç”Ÿæˆå°æ—¶æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆå°æ—¶è½¬åŒ–æŠ¥å‘Š...")
        
        try:
            if not self.db_url:
                return
            
            conn = await asyncpg.connect(self.db_url)
            
            # è·å–æœ€è¿‘1å°æ—¶çš„ç»Ÿè®¡
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_conversions,
                    SUM(usd_sale_amount) as total_sales,
                    SUM(usd_payout) as total_payout,
                    AVG(usd_sale_amount) as avg_order_value
                FROM conversions 
                WHERE received_at > NOW() - INTERVAL '1 hour'
            """)
            
            await conn.close()
            
            report = {
                'timestamp': datetime.now().isoformat(),
                'period': 'last_hour',
                'total_conversions': stats['total_conversions'] or 0,
                'total_sales': float(stats['total_sales'] or 0),
                'total_payout': float(stats['total_payout'] or 0),
                'avg_order_value': float(stats['avg_order_value'] or 0)
            }
            
            logger.info(f"ğŸ“Š å°æ—¶æŠ¥å‘Š: è½¬åŒ– {report['total_conversions']} ç¬”, "
                       f"é”€å”® ${report['total_sales']:.2f}, "
                       f"ä½£é‡‘ ${report['total_payout']:.2f}")
            
            # ä¿å­˜æŠ¥å‘Š
            with open('/tmp/hourly_conversion_reports.log', 'a') as f:
                f.write(f"{json.dumps(report)}\n")
                
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå°æ—¶æŠ¥å‘Šå¤±è´¥: {str(e)}")

async def main():
    """ä¸»å‡½æ•°"""
    monitor = ConversionMonitor()
    
    # é€‰æ‹©ç›‘æ§æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--scheduled':
        # å®šæ—¶ç›‘æ§æ¨¡å¼
        monitor.run_scheduled_monitoring()
    else:
        # æŒç»­ç›‘æ§æ¨¡å¼
        await monitor.run_continuous_monitoring()

if __name__ == "__main__":
    asyncio.run(main()) 