#!/usr/bin/env python3
"""
æ•°æ®è¿ç§»è„šæœ¬ï¼šå°†JSONæ–‡ä»¶ä¸­çš„è½¬æ¢æ•°æ®è¿ç§»åˆ°Cloud SQL PostgreSQLæ•°æ®åº“
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from decimal import Decimal
import logging
import asyncpg
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Cloud SQLè¿æ¥é…ç½®
DATABASE_URL = "postgresql://postback_admin:ByteC2024PostBack_CloudSQL_20250708@34.124.206.16:5432/postback_db"
JSON_FILE_PATH = "../complete_migration_data.json"

async def create_database_tables(conn):
    """åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„"""
    logger.info("åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„...")
    
    # åˆ›å»ºç§Ÿæˆ·è¡¨
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS tenants (
            id SERIAL PRIMARY KEY,
            tenant_code VARCHAR(50) UNIQUE NOT NULL DEFAULT 'default',
            tenant_name VARCHAR(100) NOT NULL DEFAULT 'Default Tenant',
            ts_token VARCHAR(255),
            tlm_token VARCHAR(255),
            ts_param VARCHAR(100),
            description TEXT,
            contact_email VARCHAR(255),
            contact_phone VARCHAR(50),
            max_daily_requests INTEGER DEFAULT 100000,
            enable_duplicate_check BOOLEAN DEFAULT true,
            data_retention_days INTEGER DEFAULT 7,
            is_active BOOLEAN DEFAULT true,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # åˆ›å»ºè½¬æ¢æ•°æ®è¡¨
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS conversions (
            id SERIAL PRIMARY KEY,
            tenant_id INTEGER REFERENCES tenants(id) ON DELETE CASCADE DEFAULT 1,
            
            -- æ ¸å¿ƒå­—æ®µ
            conversion_id VARCHAR(255) NOT NULL,
            offer_name TEXT,
            
            -- æ—¶é—´å­—æ®µ
            datetime_conversion TIMESTAMP WITH TIME ZONE,
            datetime_conversion_updated TIMESTAMP WITH TIME ZONE,
            
            -- é‡‘é¢å­—æ®µ
            usd_sale_amount DECIMAL(15,2),
            usd_payout DECIMAL(15,2),
            
            -- ç‚¹å‡»å’Œåª’ä½“ä¿¡æ¯
            click_id VARCHAR(255),
            media_id VARCHAR(255),
            
            -- å‘å¸ƒå•†è‡ªå®šä¹‰å‚æ•°
            aff_sub VARCHAR(255),
            aff_sub2 VARCHAR(255),
            aff_sub3 VARCHAR(255),
            
            -- å¥–åŠ±å­—æ®µ
            rewards DECIMAL(15,2),
            
            -- äº‹ä»¶ä¿¡æ¯
            event VARCHAR(255),
            event_time TIMESTAMP WITH TIME ZONE,
            
            -- å¤„ç†çŠ¶æ€
            is_processed BOOLEAN DEFAULT false,
            is_duplicate BOOLEAN DEFAULT false,
            processing_error TEXT,
            
            -- åŸå§‹æ•°æ®
            raw_data JSONB,
            raw_params JSONB,
            
            -- è¿ç§»ç›¸å…³
            migrated_from VARCHAR(50) DEFAULT 'json_file',
            original_id INTEGER,
            original_timestamp DECIMAL(20,10),
            processing_time_ms DECIMAL(10,6),
            
            -- æ—¶é—´æˆ³
            received_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # åˆ›å»ºç´¢å¼•
    await conn.execute("CREATE INDEX IF NOT EXISTS idx_conversions_tenant_id ON conversions(tenant_id);")
    await conn.execute("CREATE INDEX IF NOT EXISTS idx_conversions_conversion_id ON conversions(conversion_id);")
    await conn.execute("CREATE INDEX IF NOT EXISTS idx_conversions_received_at ON conversions(received_at);")
    await conn.execute("CREATE INDEX IF NOT EXISTS idx_conversions_aff_sub ON conversions(aff_sub);")
    await conn.execute("CREATE INDEX IF NOT EXISTS idx_conversions_datetime_conversion ON conversions(datetime_conversion);")
    await conn.execute("CREATE INDEX IF NOT EXISTS idx_conversions_original_id ON conversions(original_id);")
    
    # æ’å…¥é»˜è®¤ç§Ÿæˆ·
    await conn.execute("""
        INSERT INTO tenants (tenant_code, tenant_name, description, is_active)
        VALUES ('default', 'Default Tenant', 'Default tenant for postback data', true)
        ON CONFLICT (tenant_code) DO NOTHING;
    """)
    
    logger.info("æ•°æ®åº“è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")


def load_json_data():
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®"""
    json_path = Path(JSON_FILE_PATH)
    
    if not json_path.exists():
        logger.error(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        return []
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        if isinstance(data, dict) and 'records' in data:
            records = data['records']
        elif isinstance(data, list):
            records = data
        else:
            logger.error("ä¸æ”¯æŒçš„JSONæ•°æ®æ ¼å¼")
            return []
        
        logger.info(f"ä»JSONæ–‡ä»¶åŠ è½½äº† {len(records)} æ¡è®°å½•")
        return records
        
    except Exception as e:
        logger.error(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
        return []


def safe_decimal(value, default=None):
    """å®‰å…¨è½¬æ¢ä¸ºDecimalç±»å‹"""
    if value is None:
        return default
    try:
        if isinstance(value, str):
            if value.strip() == '' or value.lower() in ['null', 'none']:
                return default
        return Decimal(str(value))
    except:
        return default


def safe_datetime(value):
    """å®‰å…¨è½¬æ¢ä¸ºdatetimeç±»å‹"""
    if not value:
        return None
    try:
        # å°è¯•è§£æä¸åŒæ ¼å¼çš„æ—¶é—´
        formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y-%m-%d %H:%M:%S.%f',
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(value, fmt)
            except:
                continue
        
        return None
    except:
        return None


async def migrate_records(conn, records):
    """è¿ç§»è½¬æ¢è®°å½•åˆ°Cloud SQL"""
    logger.info(f"å¼€å§‹è¿ç§» {len(records)} æ¡è®°å½•åˆ°Cloud SQL...")
    
    migrated = 0
    errors = 0
    
    for i, record in enumerate(records, 1):
        try:
            data = record.get('data', {})
            
            # å‡†å¤‡æ•°æ®
            conversion_data = {
                'conversion_id': data.get('conversion_id', f'migrated_{record.get("id", i)}'),
                'offer_name': data.get('offer_name'),
                'usd_sale_amount': safe_decimal(data.get('usd_sale_amount')),
                'usd_payout': safe_decimal(data.get('usd_payout')),
                'rewards': safe_decimal(data.get('rewards')),
                'click_id': data.get('click_id'),
                'media_id': data.get('media_id'),
                'aff_sub': data.get('aff_sub'),
                'aff_sub2': data.get('aff_sub2'),
                'aff_sub3': data.get('aff_sub3'),
                'event': data.get('event'),
                'event_time': safe_datetime(data.get('event_time')),
                'datetime_conversion': safe_datetime(data.get('datetime_conversion')),
                'raw_data': json.dumps(data),
                'raw_params': json.dumps(data.get('raw_params', {})),
                'original_id': record.get('id'),
                'original_timestamp': safe_decimal(record.get('timestamp')),
                'processing_time_ms': safe_decimal(record.get('processing_time_ms')),
                'received_at': datetime.fromtimestamp(record.get('timestamp', 0)) if record.get('timestamp') else datetime.now(),
                'is_processed': True
            }
            
            # æ’å…¥æ•°æ®
            await conn.execute("""
                INSERT INTO conversions (
                    tenant_id, conversion_id, offer_name, usd_sale_amount, usd_payout,
                    rewards, click_id, media_id, aff_sub, aff_sub2, aff_sub3,
                    event, event_time, datetime_conversion, raw_data, raw_params,
                    original_id, original_timestamp, processing_time_ms,
                    received_at, is_processed, migrated_from
                ) VALUES (
                    1, $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                    $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21
                ) ON CONFLICT (conversion_id) DO UPDATE SET
                    updated_at = CURRENT_TIMESTAMP,
                    raw_data = $14,
                    raw_params = $15
            """, 
                conversion_data['conversion_id'],
                conversion_data['offer_name'],
                conversion_data['usd_sale_amount'],
                conversion_data['usd_payout'],
                conversion_data['rewards'],
                conversion_data['click_id'],
                conversion_data['media_id'],
                conversion_data['aff_sub'],
                conversion_data['aff_sub2'],
                conversion_data['aff_sub3'],
                conversion_data['event'],
                conversion_data['event_time'],
                conversion_data['datetime_conversion'],
                conversion_data['raw_data'],
                conversion_data['raw_params'],
                conversion_data['original_id'],
                conversion_data['original_timestamp'],
                conversion_data['processing_time_ms'],
                conversion_data['received_at'],
                conversion_data['is_processed'],
                'json_file'
            )
            
            migrated += 1
            
            if i % 50 == 0:
                logger.info(f"å·²è¿ç§» {i}/{len(records)} æ¡è®°å½•...")
                
        except Exception as e:
            logger.error(f"è¿ç§»ç¬¬ {i} æ¡è®°å½•å¤±è´¥: {str(e)}")
            logger.error(f"è®°å½•å†…å®¹: {record}")
            errors += 1
            continue
    
    logger.info(f"è¿ç§»å®Œæˆï¼æˆåŠŸ: {migrated} æ¡ï¼Œå¤±è´¥: {errors} æ¡")
    return migrated, errors


async def verify_migration(conn):
    """éªŒè¯è¿ç§»ç»“æœ"""
    logger.info("éªŒè¯è¿ç§»ç»“æœ...")
    
    # ç»Ÿè®¡æ€»è®°å½•æ•°
    total = await conn.fetchval("SELECT COUNT(*) FROM conversions")
    
    # ç»Ÿè®¡ä¸åŒçŠ¶æ€çš„è®°å½•
    with_amounts = await conn.fetchval(
        "SELECT COUNT(*) FROM conversions WHERE usd_sale_amount IS NOT NULL AND usd_sale_amount > 0"
    )
    
    # ç»Ÿè®¡æŒ‰aff_subåˆ†ç»„çš„è®°å½•
    aff_sub_stats = await conn.fetch(
        "SELECT aff_sub, COUNT(*) as count FROM conversions GROUP BY aff_sub ORDER BY count DESC LIMIT 10"
    )
    
    # ç»Ÿè®¡é‡‘é¢æ€»å’Œ
    total_amount = await conn.fetchval(
        "SELECT SUM(usd_sale_amount) FROM conversions WHERE usd_sale_amount IS NOT NULL"
    )
    
    total_payout = await conn.fetchval(
        "SELECT SUM(usd_payout) FROM conversions WHERE usd_payout IS NOT NULL"
    )
    
    # è·å–æ ·æœ¬æ•°æ®
    sample = await conn.fetch(
        "SELECT conversion_id, offer_name, usd_sale_amount, usd_payout, aff_sub, received_at FROM conversions ORDER BY received_at DESC LIMIT 5"
    )
    
    logger.info(f"éªŒè¯ç»“æœ:")
    logger.info(f"  æ€»è®°å½•æ•°: {total}")
    logger.info(f"  æœ‰é”€å”®é‡‘é¢çš„è®°å½•: {with_amounts}")
    logger.info(f"  æ€»é”€å”®é‡‘é¢: ${total_amount}")
    logger.info(f"  æ€»ä½£é‡‘: ${total_payout}")
    logger.info(f"  æŒ‰aff_subåˆ†ç»„ç»Ÿè®¡:")
    
    for row in aff_sub_stats:
        logger.info(f"    {row['aff_sub']}: {row['count']} æ¡")
    
    logger.info(f"  æœ€æ–°5æ¡è®°å½•:")
    for row in sample:
        logger.info(f"    {row['conversion_id']} | {row['offer_name']} | ${row['usd_sale_amount']} | {row['aff_sub']} | {row['received_at']}")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ä»JSONæ–‡ä»¶è¿ç§»æ•°æ®åˆ°Cloud SQL...")
    
    try:
        # åŠ è½½JSONæ•°æ®
        records = load_json_data()
        
        if not records:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°éœ€è¦è¿ç§»çš„æ•°æ®")
            return
        
        # è¿æ¥Cloud SQLæ•°æ®åº“
        logger.info("è¿æ¥Cloud SQLæ•°æ®åº“...")
        conn = await asyncpg.connect(DATABASE_URL)
        
        # åˆ›å»ºè¡¨ç»“æ„
        await create_database_tables(conn)
        
        # è¿ç§»æ•°æ®
        migrated, errors = await migrate_records(conn, records)
        
        # éªŒè¯ç»“æœ
        await verify_migration(conn)
        
        # å…³é—­è¿æ¥
        await conn.close()
        
        logger.info("ğŸ‰ æ•°æ®è¿ç§»å®Œæˆï¼")
        logger.info(f"ğŸ“Š è¿ç§»ç»Ÿè®¡: æˆåŠŸ {migrated} æ¡ï¼Œå¤±è´¥ {errors} æ¡")
        
    except Exception as e:
        logger.error(f"âŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 