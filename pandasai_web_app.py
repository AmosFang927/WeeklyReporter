#!/usr/bin/env python3
"""
PandasAI + Google Cloud SQL Webåº”ç”¨
æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œäº¤äº’å¼å›¾è¡¨
"""

import os
import asyncio
import asyncpg
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.utils import PlotlyJSONEncoder
import json
from decimal import Decimal
from datetime import datetime, timedelta
import streamlit as st
from typing import List, Dict, Optional
import logging
import threading
from concurrent.futures import ThreadPoolExecutor

# PandasAI ç›¸å…³å¯¼å…¥
try:
    from pandasai import SmartDataframe
    from pandasai.llm import GoogleGemini
    from pandasai.responses.response_parser import ResponseParser
    PANDASAI_AVAILABLE = True
except ImportError:
    PANDASAI_AVAILABLE = False
    st.warning("âš ï¸ PandasAI æœªå®‰è£…ï¼Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢åŠŸèƒ½å°†ä¸å¯ç”¨")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ•°æ®åº“é…ç½®
POSTBACK_DB_CONFIG = {
    "host": os.getenv("DB_HOST", "34.124.206.16"),
    "port": int(os.getenv("DB_PORT", "5432")),
    "database": os.getenv("DB_NAME", "postback_db"),
    "user": os.getenv("DB_USER", "postback_admin"),
    "password": os.getenv("DB_PASSWORD", "ByteC2024PostBack_CloudSQL_20250708")
}

# Google Gemini APIé…ç½®
GEMINI_API_KEY = os.getenv("GOOGLE_GEMINI_API_KEY", "")

class FixedGoogleGemini(GoogleGemini):
    def __init__(self, api_key: str, **kwargs):
        """ä¿®å¤æ¨¡å‹é…ç½®é¡ºåºé—®é¢˜"""
        # å…ˆè®¾ç½®å‚æ•°
        self._set_params(**kwargs)
        # ç„¶åé…ç½® API
        self._configure(api_key=api_key)
    
    def _generate_text(self, prompt: str, memory=None) -> str:
        """é‡å†™æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œæ·»åŠ å®‰å…¨è¿‡æ»¤å™¨é”™è¯¯å¤„ç†"""
        try:
            # è°ƒç”¨åŸå§‹æ–¹æ³•
            return super()._generate_text(prompt, memory)
        except ValueError as e:
            error_msg = str(e)
            if "finish_reason" in error_msg and "2" in error_msg:
                # å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢äº†å“åº”
                logger.warning(f"Google Gemini safety filter triggered: {error_msg}")
                return """
# åˆ†æç»“æœ

ç”±äºå†…å®¹å®‰å…¨ç­–ç•¥ï¼ŒAIæ— æ³•ç”Ÿæˆå“åº”ã€‚è¯·å°è¯•ï¼š

1. é‡æ–°æè¿°æ‚¨çš„é—®é¢˜
2. ä½¿ç”¨æ›´å…·ä½“çš„æ•°æ®æŸ¥è¯¢
3. é¿å…å¯èƒ½è§¦å‘å®‰å…¨è¿‡æ»¤å™¨çš„è¯æ±‡

## å»ºè®®çš„æ•°æ®æŸ¥è¯¢æ–¹å¼ï¼š

```python
# æŸ¥çœ‹æ•°æ®æ¦‚è§ˆ
df.head()

# åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
df.describe()

# æ•°æ®è®¡æ•°
df.groupby('partner_name').size()
```

result = df.head(10)  # æ˜¾ç¤ºå‰10è¡Œæ•°æ®
"""
            else:
                # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                raise e
        except Exception as e:
            logger.error(f"Google Gemini API error: {e}")
            return f"""
# æŸ¥è¯¢å¤„ç†é”™è¯¯

é‡åˆ°APIé”™è¯¯ï¼š{str(e)}

## å»ºè®®æ“ä½œï¼š

1. è¯·æ£€æŸ¥æ‚¨çš„æŸ¥è¯¢æ˜¯å¦è¿‡äºå¤æ‚
2. å°è¯•ç®€åŒ–æŸ¥è¯¢å†…å®¹
3. æ£€æŸ¥æ•°æ®æ˜¯å¦å¯ç”¨

```python
# ç®€å•çš„æ•°æ®æŸ¥çœ‹
result = df.head(10)
```
"""

class PandasAIManager:
    """PandasAIç®¡ç†å™¨ - å¤„ç†è‡ªç„¶è¯­è¨€æŸ¥è¯¢"""
    
    def __init__(self):
        self.llm = None
        self.smart_dfs = {}
        self.initialize_llm()
    
    def initialize_llm(self):
        """åˆå§‹åŒ–LLM"""
        if not PANDASAI_AVAILABLE:
            return
        
        if not GEMINI_API_KEY:
            st.error("âŒ è¯·è®¾ç½® GOOGLE_GEMINI_API_KEY ç¯å¢ƒå˜é‡")
            return
        
        try:
            # ä½¿ç”¨ä¿®å¤åçš„ç±»
            self.llm = FixedGoogleGemini(
                api_key=GEMINI_API_KEY,
                model="models/gemini-2.5-flash"  # ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹åç§°
            )
            logger.info("âœ… Google Gemini LLM åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"âœ… ä½¿ç”¨æ¨¡å‹: {self.llm.model}")
        except Exception as e:
            logger.error(f"âŒ LLM åˆå§‹åŒ–å¤±è´¥: {e}")
            st.error(f"âŒ LLM åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def create_smart_dataframe(self, df: pd.DataFrame, name: str) -> Optional[SmartDataframe]:
        """åˆ›å»ºæ™ºèƒ½æ•°æ®æ¡†"""
        if not PANDASAI_AVAILABLE or not self.llm:
            return None
        
        # æ£€æŸ¥ DataFrame æ˜¯å¦ä¸ºç©º
        if df is None or df.empty:
            logger.warning(f"âš ï¸ æ•°æ®æ¡† '{name}' ä¸ºç©ºï¼Œè·³è¿‡åˆ›å»ºæ™ºèƒ½æ•°æ®æ¡†")
            return None
        
        try:
            smart_df = SmartDataframe(df, config={
                "llm": self.llm,
                "verbose": True,
                "conversational": True,
                "save_charts": True,
                "save_charts_path": "charts/",
                "cache_path": f"cache/{name}_cache.db"  # ä¸ºæ¯ä¸ªæ•°æ®æ¡†ä½¿ç”¨ç‹¬ç«‹ç¼“å­˜
            })
            self.smart_dfs[name] = smart_df
            logger.info(f"âœ… åˆ›å»ºæ™ºèƒ½æ•°æ®æ¡† '{name}' æˆåŠŸï¼ŒåŒ…å« {len(df)} æ¡è®°å½•")
            return smart_df
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ™ºèƒ½æ•°æ®æ¡†å¤±è´¥: {e}")
            return None
    
    def query_dataframe(self, query: str, df_name: str = "main") -> tuple:
        """æŸ¥è¯¢æ•°æ®æ¡†"""
        if not PANDASAI_AVAILABLE or not self.llm:
            return None, "PandasAI ä¸å¯ç”¨"
        
        if df_name not in self.smart_dfs:
            return None, f"æ•°æ®æ¡† '{df_name}' ä¸å­˜åœ¨"
        
        try:
            smart_df = self.smart_dfs[df_name]
            if smart_df is None:
                return None, f"æ•°æ®æ¡† '{df_name}' ä¸ºç©º"
            
            result = smart_df.chat(query)
            return result, None
        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # å¦‚æœæ˜¯å®‰å…¨è¿‡æ»¤å™¨é”™è¯¯ï¼Œè¿”å›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            if "safety" in str(e).lower() or "filter" in str(e).lower():
                return None, "ç”±äºå®‰å…¨ç­–ç•¥ï¼ŒAIæ— æ³•å¤„ç†æ­¤æŸ¥è¯¢ã€‚è¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜ã€‚"
            
            return None, error_msg

class PostbackDataManager:
    """PostBackæ•°æ®ç®¡ç†å™¨ - å®æ—¶æ•°æ®åŠ è½½å’Œå¤„ç†"""
    
    def __init__(self):
        self.db_pool = None
        self._cache = {}
        self._cache_timestamp = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        self._connection_string = f"postgresql://{POSTBACK_DB_CONFIG['user']}:{POSTBACK_DB_CONFIG['password']}@{POSTBACK_DB_CONFIG['host']}:{POSTBACK_DB_CONFIG['port']}/{POSTBACK_DB_CONFIG['database']}"
        self._initialized = False
        self._lock = asyncio.Lock()
    
    def _run_async(self, coro):
        """è¿è¡Œå¼‚æ­¥å‡½æ•°çš„è¾…åŠ©æ–¹æ³• - å¤„ç†äº‹ä»¶å¾ªç¯å†²çª"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_running_loop()
                # å¦‚æœæœ‰è¿è¡Œçš„å¾ªç¯ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥å‡½æ•°
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, coro)
                    return future.result()
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œçš„å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                return asyncio.run(coro)
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥æ‰§è¡Œå¤±è´¥: {e}")
            return pd.DataFrame()
    
    async def _get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„è¿æ¥ï¼Œé¿å…è¿æ¥æ± é—®é¢˜
            conn = await asyncpg.connect(
                self._connection_string,
                command_timeout=30
            )
            return conn
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def _is_cache_valid(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if key not in self._cache_timestamp:
            return False
        return (datetime.now() - self._cache_timestamp[key]).seconds < self.cache_ttl
    
    def _convert_decimal_to_float(self, data: List[Dict]) -> List[Dict]:
        """è½¬æ¢Decimalç±»å‹ä¸ºfloat"""
        converted_data = []
        for row in data:
            converted_row = {}
            for key, value in row.items():
                if isinstance(value, Decimal):
                    converted_row[key] = float(value)
                else:
                    converted_row[key] = value
            converted_data.append(converted_row)
        return converted_data
    
    def get_comprehensive_data(self, days: int = 7) -> pd.DataFrame:
        """è·å–ç»¼åˆæ•°æ®ç”¨äºPandasAIæŸ¥è¯¢"""
        return self._run_async(self._get_comprehensive_data(days))
    
    async def _get_comprehensive_data(self, days: int = 7) -> pd.DataFrame:
        """è·å–ç»¼åˆæ•°æ®çš„å¼‚æ­¥ç‰ˆæœ¬"""
        cache_key = f"comprehensive_data_{days}"
        
        if self._is_cache_valid(cache_key):
            return self._cache[cache_key]
        
        conn = None
        try:
            conn = await self._get_db_connection()
            
            query = """
            SELECT 
                id,
                tenant_id,
                conversion_id,
                offer_name,
                usd_sale_amount,
                usd_payout,
                aff_sub,
                event_time,
                created_at,
                DATE(created_at) as conversion_date,
                EXTRACT(HOUR FROM created_at) as conversion_hour,
                EXTRACT(DOW FROM created_at) as day_of_week,
                CASE 
                    WHEN EXTRACT(DOW FROM created_at) IN (0, 6) THEN 'Weekend'
                    ELSE 'Weekday'
                END as day_type,
                raw_data->>'click_id' as click_id,
                raw_data->>'media_id' as media_id,
                raw_data->>'sub_id' as sub_id_raw
            FROM conversions 
            WHERE created_at >= CURRENT_DATE - INTERVAL '{} days'
            ORDER BY created_at DESC
            LIMIT 1000
            """.format(days)
            
            result = await conn.fetch(query)
            data = self._convert_decimal_to_float([dict(row) for row in result])
                
            df = pd.DataFrame(data)
            if not df.empty:
                df['created_at'] = pd.to_datetime(df['created_at'])
                df['conversion_date'] = pd.to_datetime(df['conversion_date'])
                # ä½¿ç”¨ aff_sub ä½œä¸ºä¸»è¦çš„ sub_id å­—æ®µ
                df['sub_id'] = df['aff_sub']
            
            self._cache[cache_key] = df
            self._cache_timestamp[cache_key] = datetime.now()
            
            logger.info(f"âœ… è·å–ç»¼åˆæ•°æ®: {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»¼åˆæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                await conn.close()
    
    def get_today_summary(self) -> pd.DataFrame:
        """è·å–ä»Šæ—¥æ±‡æ€»æ•°æ®"""
        return self._run_async(self._get_today_summary())
    
    async def _get_today_summary(self) -> pd.DataFrame:
        """è·å–ä»Šæ—¥æ±‡æ€»æ•°æ®çš„å¼‚æ­¥ç‰ˆæœ¬"""
        cache_key = "today_summary"
        
        if self._is_cache_valid(cache_key):
            return self._cache[cache_key]
        
        conn = None
        try:
            conn = await self._get_db_connection()
            
            query = """
            SELECT 
                DATE(created_at) as date,
                offer_name,
                COUNT(*) as conversion_count,
                SUM(usd_payout) as total_revenue,
                AVG(usd_payout) as avg_payout,
                MIN(usd_payout) as min_payout,
                MAX(usd_payout) as max_payout,
                COUNT(DISTINCT aff_sub) as unique_sub_ids
            FROM conversions 
            WHERE DATE(created_at) = CURRENT_DATE
                AND usd_payout IS NOT NULL
            GROUP BY DATE(created_at), offer_name
            ORDER BY conversion_count DESC
            """
            
            result = await conn.fetch(query)
            data = self._convert_decimal_to_float([dict(row) for row in result])
                
            df = pd.DataFrame(data)
            self._cache[cache_key] = df
            self._cache_timestamp[cache_key] = datetime.now()
            
            logger.info(f"âœ… è·å–ä»Šæ—¥æ±‡æ€»æ•°æ®: {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å–ä»Šæ—¥æ±‡æ€»æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                await conn.close()
    
    def get_hourly_trend(self, days: int = 7) -> pd.DataFrame:
        """è·å–æŒ‰å°æ—¶çš„è½¬åŒ–è¶‹åŠ¿"""
        return self._run_async(self._get_hourly_trend(days))
    
    async def _get_hourly_trend(self, days: int = 7) -> pd.DataFrame:
        """è·å–æŒ‰å°æ—¶çš„è½¬åŒ–è¶‹åŠ¿çš„å¼‚æ­¥ç‰ˆæœ¬"""
        cache_key = f"hourly_trend_{days}"
        
        if self._is_cache_valid(cache_key):
            return self._cache[cache_key]
        
        conn = None
        try:
            conn = await self._get_db_connection()
            
            query = """
            SELECT 
                DATE(created_at) as date,
                EXTRACT(HOUR FROM created_at) as hour,
                COUNT(*) as conversion_count,
                SUM(COALESCE(usd_payout, 0)) as total_revenue
            FROM conversions 
            WHERE created_at >= CURRENT_DATE - INTERVAL '{} days'
            GROUP BY DATE(created_at), EXTRACT(HOUR FROM created_at)
            ORDER BY date, hour
            """.format(days)
            
            result = await conn.fetch(query)
            data = self._convert_decimal_to_float([dict(row) for row in result])
                
            df = pd.DataFrame(data)
            self._cache[cache_key] = df
            self._cache_timestamp[cache_key] = datetime.now()
            
            logger.info(f"âœ… è·å–å°æ—¶è¶‹åŠ¿æ•°æ®: {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å–å°æ—¶è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                await conn.close()
    
    def get_partner_performance(self, days: int = 7) -> pd.DataFrame:
        """è·å–Sub IDè¡¨ç°æ•°æ®"""
        return self._run_async(self._get_partner_performance(days))
    
    async def _get_partner_performance(self, days: int = 7) -> pd.DataFrame:
        """è·å–Sub IDè¡¨ç°æ•°æ®çš„å¼‚æ­¥ç‰ˆæœ¬"""
        cache_key = f"partner_performance_{days}"
        
        if self._is_cache_valid(cache_key):
            return self._cache[cache_key]
        
        conn = None
        try:
            conn = await self._get_db_connection()
            
            query = """
            SELECT 
                aff_sub as sub_id,
                COUNT(*) as conversion_count,
                SUM(COALESCE(usd_payout, 0)) as total_revenue,
                AVG(COALESCE(usd_payout, 0)) as avg_payout,
                COUNT(DISTINCT offer_name) as unique_offers,
                MIN(created_at) as first_conversion,
                MAX(created_at) as last_conversion
            FROM conversions 
            WHERE created_at >= CURRENT_DATE - INTERVAL '{} days'
                AND aff_sub IS NOT NULL 
                AND aff_sub != ''
            GROUP BY aff_sub
            HAVING COUNT(*) >= 2
            ORDER BY conversion_count DESC
            LIMIT 20
            """.format(days)
            
            result = await conn.fetch(query)
            data = self._convert_decimal_to_float([dict(row) for row in result])
                
            df = pd.DataFrame(data)
            self._cache[cache_key] = df
            self._cache_timestamp[cache_key] = datetime.now()
            
            logger.info(f"âœ… è·å–Sub IDè¡¨ç°æ•°æ®: {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å–Sub IDè¡¨ç°æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                await conn.close()

    def get_country_analysis(self, days: int = 7) -> pd.DataFrame:
        """è·å–åœ°åŒºåˆ†ææ•°æ®"""
        return self._run_async(self._get_country_analysis(days))
    
    async def _get_country_analysis(self, days: int = 7) -> pd.DataFrame:
        """è·å–åœ°åŒºåˆ†ææ•°æ®çš„å¼‚æ­¥ç‰ˆæœ¬"""
        cache_key = f"country_analysis_{days}"
        
        if self._is_cache_valid(cache_key):
            return self._cache[cache_key]
        
        conn = None
        try:
            conn = await self._get_db_connection()
            
            query = """
            SELECT 
                CASE 
                    WHEN offer_name LIKE '%MY%' THEN 'Malaysia'
                    WHEN offer_name LIKE '%ID%' THEN 'Indonesia'
                    WHEN offer_name LIKE '%TH%' THEN 'Thailand'
                    WHEN offer_name LIKE '%SG%' THEN 'Singapore'
                    WHEN offer_name LIKE '%PH%' THEN 'Philippines'
                    WHEN offer_name LIKE '%VN%' THEN 'Vietnam'
                    ELSE 'Other'
                END as region,
                COUNT(*) as conversion_count,
                SUM(COALESCE(usd_payout, 0)) as total_revenue,
                AVG(COALESCE(usd_payout, 0)) as avg_payout,
                COUNT(DISTINCT offer_name) as unique_offers,
                COUNT(DISTINCT aff_sub) as unique_sub_ids
            FROM conversions 
            WHERE created_at >= CURRENT_DATE - INTERVAL '{} days'
            GROUP BY CASE 
                WHEN offer_name LIKE '%MY%' THEN 'Malaysia'
                WHEN offer_name LIKE '%ID%' THEN 'Indonesia'
                WHEN offer_name LIKE '%TH%' THEN 'Thailand'
                WHEN offer_name LIKE '%SG%' THEN 'Singapore'
                WHEN offer_name LIKE '%PH%' THEN 'Philippines'
                WHEN offer_name LIKE '%VN%' THEN 'Vietnam'
                ELSE 'Other'
            END
            HAVING COUNT(*) >= 1
            ORDER BY conversion_count DESC
            """.format(days)
            
            result = await conn.fetch(query)
            data = self._convert_decimal_to_float([dict(row) for row in result])
                
            df = pd.DataFrame(data)
            self._cache[cache_key] = df
            self._cache_timestamp[cache_key] = datetime.now()
            
            logger.info(f"âœ… è·å–åœ°åŒºåˆ†ææ•°æ®: {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å–åœ°åŒºåˆ†ææ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                await conn.close()

class ChartGenerator:
    """å›¾è¡¨ç”Ÿæˆå™¨ - ä½¿ç”¨Plotlyåˆ›å»ºäº¤äº’å¼å›¾è¡¨"""
    
    @staticmethod
    def create_offer_performance_chart(df: pd.DataFrame) -> dict:
        """åˆ›å»ºOfferè¡¨ç°å›¾è¡¨"""
        if df.empty:
            return {}
        
        fig = go.Figure()
        
        # è½¬åŒ–æ•°é‡æŸ±çŠ¶å›¾
        fig.add_trace(go.Bar(
            name='è½¬åŒ–æ•°é‡',
            x=df['offer_name'],
            y=df['conversion_count'],
            yaxis='y',
            marker_color='rgb(55, 83, 109)'
        ))
        
        # æ”¶å…¥çº¿å›¾
        fig.add_trace(go.Scatter(
            name='æ€»æ”¶å…¥',
            x=df['offer_name'],
            y=df['total_revenue'],
            yaxis='y2',
            mode='lines+markers',
            line=dict(color='rgb(26, 118, 255)')
        ))
        
        fig.update_layout(
            title='Offerè¡¨ç°åˆ†æ',
            xaxis=dict(title='Offeråç§°', tickangle=45),
            yaxis=dict(title='è½¬åŒ–æ•°é‡', side='left'),
            yaxis2=dict(title='æ€»æ”¶å…¥ ($)', side='right', overlaying='y'),
            hovermode='x unified',
            legend=dict(x=0.01, y=0.99),
            height=600
        )
        
        return json.loads(PlotlyJSONEncoder().encode(fig))
    
    @staticmethod
    def create_hourly_trend_chart(df: pd.DataFrame) -> dict:
        """åˆ›å»ºå°æ—¶è¶‹åŠ¿å›¾è¡¨"""
        if df.empty:
            return {}
        
        # åˆ›å»ºå®Œæ•´çš„æ—¥æœŸæ—¶é—´
        df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['hour'], unit='h')
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=df['datetime'],
            y=df['conversion_count'],
            mode='lines+markers',
            name='è½¬åŒ–æ•°é‡',
            line=dict(color='rgb(55, 83, 109)'),
            hovertemplate='æ—¶é—´: %{x}<br>è½¬åŒ–æ•°: %{y}<extra></extra>'
        ))
        
        fig.update_layout(
            title='è½¬åŒ–è¶‹åŠ¿ - æŒ‰å°æ—¶',
            xaxis=dict(title='æ—¶é—´'),
            yaxis=dict(title='è½¬åŒ–æ•°é‡'),
            hovermode='x unified',
            height=500
        )
        
        return json.loads(PlotlyJSONEncoder().encode(fig))
    
    @staticmethod
    def create_partner_comparison_chart(df: pd.DataFrame) -> dict:
        """åˆ›å»ºSub IDæ¯”è¾ƒå›¾è¡¨"""
        if df.empty:
            return {}
        
        # å–å‰10ä¸ªSub ID
        df_top = df.head(10)
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name='è½¬åŒ–æ•°é‡',
            x=df_top['sub_id'],
            y=df_top['conversion_count'],
            marker_color='rgb(158, 185, 243)',
            hovertemplate='Sub ID: %{x}<br>è½¬åŒ–æ•°: %{y}<extra></extra>'
        ))
        
        fig.update_layout(
            title='Top 10 Sub IDè¡¨ç°',
            xaxis=dict(title='Sub ID', tickangle=45),
            yaxis=dict(title='è½¬åŒ–æ•°é‡'),
            height=500
        )
        
        return json.loads(PlotlyJSONEncoder().encode(fig))
    
    @staticmethod
    def create_country_pie_chart(df: pd.DataFrame) -> dict:
        """åˆ›å»ºåœ°åŒºåˆ†å¸ƒé¥¼å›¾"""
        if df.empty:
            return {}
        
        # å–å‰8ä¸ªåœ°åŒºï¼Œå…¶ä½™å½’ä¸º"å…¶ä»–"
        df_top = df.head(8)
        others_count = df.iloc[8:]['conversion_count'].sum() if len(df) > 8 else 0
        
        if others_count > 0:
            others_row = pd.DataFrame([{'region': 'å…¶ä»–', 'conversion_count': others_count}])
            df_chart = pd.concat([df_top, others_row], ignore_index=True)
        else:
            df_chart = df_top
        
        fig = go.Figure(data=[go.Pie(
            labels=df_chart['region'],
            values=df_chart['conversion_count'],
            hole=0.4,
            hovertemplate='åœ°åŒº: %{label}<br>è½¬åŒ–æ•°: %{value}<br>å æ¯”: %{percent}<extra></extra>'
        )])
        
        fig.update_layout(
            title='è½¬åŒ–åˆ†å¸ƒ - æŒ‰åœ°åŒº',
            height=500
        )
        
        return json.loads(PlotlyJSONEncoder().encode(fig))

# Streamlitåº”ç”¨ä¸»å‡½æ•°
def main():
    """Streamlitä¸»åº”ç”¨"""
    st.set_page_config(
        page_title="PostBack Analytics Dashboard",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ“Š PostBack Analytics Dashboard")
    st.markdown("### ğŸ¤– åŸºäºPandasAIçš„æ™ºèƒ½æ•°æ®åˆ†æå¹³å°")
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    if 'data_manager' not in st.session_state:
        st.session_state.data_manager = PostbackDataManager()
    
    if 'pandasai_manager' not in st.session_state:
        st.session_state.pandasai_manager = PandasAIManager()
    
    data_manager = st.session_state.data_manager
    pandasai_manager = st.session_state.pandasai_manager
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.title("âš™ï¸ é…ç½®é€‰é¡¹")
    
    # APIå¯†é’¥è®¾ç½®
    if PANDASAI_AVAILABLE:
        st.sidebar.subheader("ğŸ”‘ APIé…ç½®")
        api_key_input = st.sidebar.text_input(
            "Google Gemini API Key",
            value=GEMINI_API_KEY,
            type="password",
            help="è¯·è¾“å…¥æ‚¨çš„ Google Gemini API Key"
        )
        
        if api_key_input != GEMINI_API_KEY:
            os.environ["GOOGLE_GEMINI_API_KEY"] = api_key_input
            pandasai_manager.initialize_llm()
    
    # æ—¶é—´èŒƒå›´é€‰æ‹©
    days_range = st.sidebar.selectbox(
        "ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´",
        [1, 3, 7, 14, 30],
        index=2,
        format_func=lambda x: f"æœ€è¿‘ {x} å¤©"
    )
    
    # åˆ·æ–°æŒ‰é’®
    if st.sidebar.button("ğŸ”„ åˆ·æ–°æ•°æ®"):
        # æ¸…é™¤ç¼“å­˜
        st.session_state.data_manager._cache.clear()
        st.session_state.data_manager._cache_timestamp.clear()
        st.rerun()
    
    # æ•°æ®åŠ è½½
    with st.spinner("ğŸ“¡ æ­£åœ¨åŠ è½½æ•°æ®..."):
        try:
            # è·å–å„ç§æ•°æ®
            comprehensive_data = data_manager.get_comprehensive_data(days_range)
            today_summary = data_manager.get_today_summary()
            hourly_trend = data_manager.get_hourly_trend(days_range)
            sub_id_performance = data_manager.get_partner_performance(days_range)
            region_analysis = data_manager.get_country_analysis(days_range)
            
            # åˆ›å»ºPandasAIæ™ºèƒ½æ•°æ®æ¡†
            if PANDASAI_AVAILABLE and not comprehensive_data.empty:
                pandasai_manager.create_smart_dataframe(comprehensive_data, "main")
                pandasai_manager.create_smart_dataframe(today_summary, "today")
                pandasai_manager.create_smart_dataframe(sub_id_performance, "sub_ids")
                pandasai_manager.create_smart_dataframe(region_analysis, "regions")
            
        except Exception as e:
            st.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return
    
    # è‡ªç„¶è¯­è¨€æŸ¥è¯¢åŒºåŸŸï¼ˆç½®é¡¶ï¼‰
    st.subheader("ğŸ¤– æ™ºèƒ½æŸ¥è¯¢åŠ©æ‰‹")
    
    if PANDASAI_AVAILABLE and pandasai_manager.llm:
        # æŸ¥è¯¢è¾“å…¥
        col1, col2 = st.columns([4, 1])
        
        with col1:
            query_input = st.text_input(
                "ğŸ’¬ ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‚¨æƒ³è¦çš„åˆ†æ...",
                placeholder="ä¾‹å¦‚ï¼šä»Šå¤©å“ªä¸ªofferè¡¨ç°æœ€å¥½ï¼Ÿæˆ–è€… æœ€è¿‘7å¤©çš„è½¬åŒ–è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"
            )
        
        with col2:
            query_button = st.button("ğŸ” æŸ¥è¯¢", type="primary")
        
        # å¿«é€ŸæŸ¥è¯¢æŒ‰é’®
        st.write("ğŸ’¡ å¿«é€ŸæŸ¥è¯¢ï¼š")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ“Š ä»Šæ—¥æœ€ä½³offer"):
                query_input = "ä»Šå¤©å“ªä¸ªofferè½¬åŒ–æ•°é‡æœ€å¤šï¼Ÿ"
                query_button = True
        
        with col2:
            if st.button("ğŸ’° æ”¶å…¥æœ€é«˜Sub ID"):
                query_input = "å“ªä¸ªSub IDæ”¶å…¥æœ€é«˜ï¼Ÿ"
                query_button = True
        
        with col3:
            if st.button("ğŸŒ è½¬åŒ–æœ€å¤šåœ°åŒº"):
                query_input = "å“ªä¸ªåœ°åŒºè½¬åŒ–æ•°é‡æœ€å¤šï¼Ÿ"
                query_button = True
        
        with col4:
            if st.button("ğŸ“ˆ å°æ—¶è¶‹åŠ¿åˆ†æ"):
                query_input = "ä¸€å¤©ä¸­å“ªä¸ªå°æ—¶è½¬åŒ–æœ€å¤šï¼Ÿ"
                query_button = True
        
        # æ‰§è¡ŒæŸ¥è¯¢
        if query_button and query_input:
            with st.spinner("ğŸ¤” AIæ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜..."):
                result, error = pandasai_manager.query_dataframe(query_input, "main")
                
                if error:
                    st.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {error}")
                else:
                    st.success("âœ… æŸ¥è¯¢å®Œæˆï¼")
                    
                    # æ˜¾ç¤ºç»“æœ
                    if isinstance(result, pd.DataFrame):
                        st.dataframe(result, use_container_width=True)
                    elif isinstance(result, str):
                        st.write(result)
                    else:
                        st.write(result)
                    
                    # å¦‚æœç”Ÿæˆäº†å›¾è¡¨ï¼Œæ˜¾ç¤ºå›¾è¡¨
                    charts_path = "charts/"
                    if os.path.exists(charts_path):
                        chart_files = [f for f in os.listdir(charts_path) if f.endswith('.png')]
                        if chart_files:
                            latest_chart = max(chart_files, key=lambda x: os.path.getctime(os.path.join(charts_path, x)))
                            st.image(os.path.join(charts_path, latest_chart), caption="AIç”Ÿæˆçš„å›¾è¡¨")
    
    else:
        st.info("ğŸš§ è¯·é…ç½® Google Gemini API Key ä»¥å¯ç”¨æ™ºèƒ½æŸ¥è¯¢åŠŸèƒ½")
        st.write("ğŸ’¡ ç¤ºä¾‹æŸ¥è¯¢ï¼š")
        st.write("- ä»Šå¤©å“ªä¸ªofferè¡¨ç°æœ€å¥½ï¼Ÿ")
        st.write("- æœ€è¿‘7å¤©çš„è½¬åŒ–è¶‹åŠ¿å¦‚ä½•ï¼Ÿ")
        st.write("- å“ªä¸ªSub IDæ”¶å…¥æœ€é«˜ï¼Ÿ")
        st.write("- å“ªä¸ªåœ°åŒºè½¬åŒ–æ•°é‡æœ€å¤šï¼Ÿ")
        st.write("- å·¥ä½œæ—¥å’Œå‘¨æœ«çš„è½¬åŒ–å¯¹æ¯”å¦‚ä½•ï¼Ÿ")
    
    # å…³é”®æŒ‡æ ‡å±•ç¤º
    if not today_summary.empty:
        st.subheader("ğŸ“Š ä»Šæ—¥å…³é”®æŒ‡æ ‡")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_conversions = today_summary['conversion_count'].sum()
            st.metric("ğŸ¯ ä»Šæ—¥è½¬åŒ–æ•°", f"{total_conversions:,}")
        
        with col2:
            total_revenue = today_summary['total_revenue'].sum()
            st.metric("ğŸ’° ä»Šæ—¥æ”¶å…¥", f"${total_revenue:.2f}")
        
        with col3:
            avg_payout = today_summary['avg_payout'].mean()
            st.metric("ğŸ“ˆ å¹³å‡å•ä»·", f"${avg_payout:.3f}")
        
        with col4:
            unique_offers = len(today_summary)
            st.metric("ğŸª æ´»è·ƒOffer", f"{unique_offers}")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š Offeråˆ†æ", "ğŸ“ˆ è¶‹åŠ¿å›¾è¡¨", "ğŸ¤ Sub IDåˆ†æ", "ğŸŒ åœ°åŒºåˆ†æ"])
    
    with tab1:
        st.subheader("ğŸ¯ Offerè¡¨ç°åˆ†æ")
        if not today_summary.empty:
            chart_json = ChartGenerator.create_offer_performance_chart(today_summary)
            if chart_json:
                st.plotly_chart(chart_json, use_container_width=True)
            
            # æ•°æ®è¡¨æ ¼
            st.subheader("ğŸ“‹ è¯¦ç»†æ•°æ®")
            st.dataframe(today_summary, use_container_width=True)
        else:
            st.info("ğŸ“ æš‚æ— ä»Šæ—¥æ•°æ®")
    
    with tab2:
        st.subheader("ğŸ“ˆ è½¬åŒ–è¶‹åŠ¿åˆ†æ")
        if not hourly_trend.empty:
            chart_json = ChartGenerator.create_hourly_trend_chart(hourly_trend)
            if chart_json:
                st.plotly_chart(chart_json, use_container_width=True)
        else:
            st.info("ğŸ“ æš‚æ— è¶‹åŠ¿æ•°æ®")
    
    with tab3:
        st.subheader("ğŸ¤ Sub IDè¡¨ç°")
        if not sub_id_performance.empty:
            chart_json = ChartGenerator.create_partner_comparison_chart(sub_id_performance)
            if chart_json:
                st.plotly_chart(chart_json, use_container_width=True)
            
            # Sub IDè¯¦ç»†æ•°æ®
            st.subheader("ğŸ“Š Sub IDè¯¦æƒ…")
            st.dataframe(sub_id_performance, use_container_width=True)
        else:
            st.info("ğŸ“ æš‚æ— Sub IDæ•°æ®")
    
    with tab4:
        st.subheader("ğŸŒ åœ°åŒºåˆ†æ")
        if not region_analysis.empty:
            chart_json = ChartGenerator.create_country_pie_chart(region_analysis)
            if chart_json:
                st.plotly_chart(chart_json, use_container_width=True)
            
            # åœ°åŒºæ•°æ®è¡¨æ ¼
            st.subheader("ğŸ—ºï¸ åœ°åŒºè¯¦æƒ…")
            st.dataframe(region_analysis, use_container_width=True)
        else:
            st.info("ğŸ“ æš‚æ— åœ°åŒºæ•°æ®")

if __name__ == "__main__":
    # è¿è¡ŒStreamlitåº”ç”¨
    main() 